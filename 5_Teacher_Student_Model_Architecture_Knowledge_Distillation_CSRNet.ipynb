{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Models**"
      ],
      "metadata": {
        "id": "BEnLDvkQvCdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Model_VGG**"
      ],
      "metadata": {
        "id": "aKrI-UvKwD_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For training CSRNet teacher\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import models\n",
        "# from utils import save_net,load_net\n",
        "import time\n",
        "\n",
        "\n",
        "class CSRNet(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(CSRNet, self).__init__()\n",
        "        self.seen = 0\n",
        "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
        "        self.backend_feat = [512, 512, 512, 256, 128, 64]\n",
        "        self.frontend = make_layers(self.frontend_feat)\n",
        "        # cal_para(self.frontend)\n",
        "        self.backend = make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
        "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
        "        if pretrained:\n",
        "            self._initialize_weights(mode='normal')\n",
        "            mod = models.vgg16(pretrained=True)\n",
        "            state_keys = list(self.frontend.state_dict().keys())\n",
        "            pretrain_keys = list(mod.state_dict().keys())\n",
        "            for i in range(len(self.frontend.state_dict().items())):\n",
        "                # self.frontend.state_dict().items()[i][1].data[:] = mod.state_dict().items()[i][1].data[:]\n",
        "                # print(mod.state_dict()[pretrain_keys[i]])\n",
        "                self.frontend.state_dict()[state_keys[i]].data = mod.state_dict()[pretrain_keys[i]].data\n",
        "        else:\n",
        "            self._initialize_weights(mode='kaiming')\n",
        "\n",
        "                \n",
        "    def forward(self, x):\n",
        "        # front relates to VGG\n",
        "        x = self.frontend(x)\n",
        "        # backend relates to dilated convolution\n",
        "        x = self.backend(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self, mode):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if mode == 'normal':\n",
        "                    nn.init.normal_(m.weight, std=0.01)\n",
        "                elif mode == 'kaiming':\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            \n",
        "                \n",
        "def make_layers(cfg, in_channels=3, batch_norm=False, dilation=False):\n",
        "    if dilation:\n",
        "        d_rate = 2\n",
        "    else:\n",
        "        d_rate = 1\n",
        "    layers = []\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "ddcnES_TwAcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Model_Teacher_VGG**"
      ],
      "metadata": {
        "id": "p6svFmbiwqPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Teacher model in SKT\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import models\n",
        "#from utils import save_net, load_net, cal_para\n",
        "\n",
        "\n",
        "class CSRNet_teacher(nn.Module):\n",
        "    \n",
        "    def __init__(self, pretrained=False):\n",
        "        super(CSRNet_teacher, self).__init__()\n",
        "        self.seen = 0\n",
        "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
        "        self.backend_feat = [512, 512, 512, 256, 128, 64]\n",
        "        self.frontend = make_layers(self.frontend_feat)\n",
        "        self.backend = make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
        "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
        "        self._initialize_weights()\n",
        "        self.features = []\n",
        "        if pretrained:\n",
        "            print ('load vgg pretrained model')\n",
        "            mod = models.vgg16(pretrained=True)\n",
        "            for i in range(len(self.frontend.state_dict().items())):\n",
        "                self.frontend.state_dict().items()[i][1].data[:] = mod.state_dict().items()[i][1].data[:]\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.features = []\n",
        "        # frontend: VGG\n",
        "        x = self.frontend(x)\n",
        "        # backend: dilated convolution\n",
        "        x = self.backend(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def regist_hook(self):\n",
        "        self.features = []\n",
        "\n",
        "        def get(model, input, output):\n",
        "            # function will be automatically called each time, since the hook is injected\n",
        "            self.features.append(output.detach())\n",
        "\n",
        "        for name, module in self._modules['frontend']._modules.items():\n",
        "            if name in ['1', '4', '9', '16']:\n",
        "                self._modules['frontend']._modules[name].register_forward_hook(get)\n",
        "        for name, module in self._modules['backend']._modules.items():\n",
        "            if name in ['1', '7']:\n",
        "                self._modules['backend']._modules[name].register_forward_hook(get)\n",
        "\n",
        "\n",
        "def make_layers(cfg, in_channels=3, batch_norm=False, dilation=False):\n",
        "    if dilation:\n",
        "        d_rate = 2\n",
        "    else:\n",
        "        d_rate = 1\n",
        "    layers = []\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "3GqLMiYQwjV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_json = \"/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/part_A_final/json/part_A_train.json\"\n",
        "val_json = \"/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/part_A_final/json/part_A_val.json\"\n",
        "test_json = \"/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/part_A_final/json/part_A_test.json\"\n",
        "task = \"A\"\n",
        "\n",
        "teacher_model = CSRNet_teacher()\n",
        "weight = torch.load('/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/checkpoint.pth.tar', \\\n",
        "                    map_location=torch.device('cpu'))['state_dict']\n",
        "teacher_model.load_state_dict(weight)\n",
        "pre = teacher_model.load_state_dict(weight)\n",
        "\n",
        "mae_best_prec1 = 1e6\n",
        "mse_best_prec1 = 1e6\n",
        "batch_size = 6\n",
        "momentum = 0.95\n",
        "decay = 1 * 1e-4  # 5 * 1e-4\n",
        "start_epoch = 0\n",
        "epochs = 1000\n",
        "workers = 0  #4\n",
        "seed = time.time()\n",
        "print_freq = 400   #30\n",
        "\n",
        "\n",
        "original_lr = 1e-4  #1e-4\n",
        "lr = 1e-7       #1e-4\n",
        "steps = [-1,1,100,150]\n",
        "scales = [1,1,1,1]\n",
        "teacher_ckpt =\"/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/checkpoint.pth.tar\"\n",
        "student_ckpt = None\n",
        "\n",
        "lamb_fsp = 0.5\n",
        "lamb_cos = 0.5\n",
        "out = \"/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/CSRNet_models_weights/partA_student.pth.t\""
      ],
      "metadata": {
        "id": "i2mxnj1bzPFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parser = argparse.ArgumentParser(description='CSRNet-SKT distillation')\n",
        "# parser.add_argument('train_json', metavar='TRAIN',\n",
        "#                     help='path to train json')\n",
        "# parser.add_argument('val_json', metavar='VAL',\n",
        "#                     help='path to val json')\n",
        "# parser.add_argument('test_json', metavar='TEST',\n",
        "#                     help='path to test json')\n",
        "# parser.add_argument('--lr', default=None, type=float,\n",
        "#                     help='learning rate')\n",
        "# # parser.add_argument('--teacher', '-t', default=None, type=str,\n",
        "# #                     help='teacher net version')\n",
        "# parser.add_argument('--teacher_ckpt', '-tc', default=None, type=str,\n",
        "#                     help='teacher checkpoint')\n",
        "# # parser.add_argument('--student', '-s', default=None, type=str,\n",
        "# #                     help='student net version')\n",
        "# parser.add_argument('--student_ckpt', '-sc', default=None, type=str,\n",
        "#                     help='student checkpoint')\n",
        "# parser.add_argument('--lamb_fsp', '-laf', type=float, default=None,\n",
        "#                     help='weight of dense fsp loss')\n",
        "# parser.add_argument('--lamb_cos', '-lac', type=float, default=None,\n",
        "#                     help='weight of cos loss')\n",
        "# parser.add_argument('--gpu', metavar='GPU', type=str, default='0',\n",
        "#                     help='GPU id to use')\n",
        "# parser.add_argument('--out', metavar='OUTPUT', type=str,\n",
        "#                     help='path to output')"
      ],
      "metadata": {
        "id": "2oed_yQpxqyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Model_Student_VGG**"
      ],
      "metadata": {
        "id": "vOBQbbR9xiJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Student model (1/n-CSRNet) in SKT.\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "channel_nums = [[32, 64, 128, 256],  # half\n",
        "                [21, 43, 85, 171],  # third\n",
        "                [16, 32, 64, 128],  # quarter\n",
        "                [13, 26, 51, 102],  # fifth\n",
        "                ]\n",
        "\n",
        "class CSRNet_student(nn.Module):\n",
        "    def __init__(self, ratio=4, transform=True):\n",
        "        super(CSRNet_student, self).__init__()\n",
        "        self.seen = 0\n",
        "        self.transform = transform\n",
        "        channel = channel_nums[ratio-2]\n",
        "        self.conv0_0 = conv_layers(3, channel[0])\n",
        "        if self.transform:\n",
        "            self.transform0_0 = feature_transform(channel[0], 64)\n",
        "        self.conv0_1 = conv_layers(channel[0], channel[0])\n",
        "\n",
        "        self.pool0 = pool_layers()\n",
        "        if transform:\n",
        "            self.transform1_0 = feature_transform(channel[0], 64)\n",
        "        self.conv1_0 = conv_layers(channel[0], channel[1])\n",
        "        self.conv1_1 = conv_layers(channel[1], channel[1])\n",
        "\n",
        "        self.pool1 = pool_layers()\n",
        "        if transform:\n",
        "            self.transform2_0 = feature_transform(channel[1], 128)\n",
        "        self.conv2_0 = conv_layers(channel[1], channel[2])\n",
        "        self.conv2_1 = conv_layers(channel[2], channel[2])\n",
        "        self.conv2_2 = conv_layers(channel[2], channel[2])\n",
        "\n",
        "        self.pool2 = pool_layers()\n",
        "        if transform:\n",
        "            self.transform3_0 = feature_transform(channel[2], 256)\n",
        "        self.conv3_0 = conv_layers(channel[2], channel[3])\n",
        "        self.conv3_1 = conv_layers(channel[3], channel[3])\n",
        "        self.conv3_2 = conv_layers(channel[3], channel[3])\n",
        "\n",
        "        self.conv4_0 = conv_layers(channel[3], channel[3], dilation=2)\n",
        "        if transform:\n",
        "            self.transform4_0 = feature_transform(channel[3], 512)\n",
        "        self.conv4_1 = conv_layers(channel[3], channel[3], dilation=2)\n",
        "        self.conv4_2 = conv_layers(channel[3], channel[3], dilation=2)\n",
        "        self.conv4_3 = conv_layers(channel[3], channel[2], dilation=2)\n",
        "        if transform:\n",
        "            self.transform4_3 = feature_transform(channel[2], 256)\n",
        "        self.conv4_4 = conv_layers(channel[2], channel[1], dilation=2)\n",
        "        self.conv4_5 = conv_layers(channel[1], channel[0], dilation=2)\n",
        "\n",
        "        self.conv5_0 = nn.Conv2d(channel[0], 1, kernel_size=1)\n",
        "\n",
        "        self._initialize_weights()\n",
        "        self.features = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.features = []\n",
        "\n",
        "        x = self.conv0_0(x)\n",
        "        if self.transform:\n",
        "            self.features.append(self.transform0_0(x))\n",
        "        x = self.conv0_1(x)\n",
        "\n",
        "        x = self.pool0(x)\n",
        "        if self.transform:\n",
        "            self.features.append(self.transform1_0(x))\n",
        "        x = self.conv1_0(x)\n",
        "        x = self.conv1_1(x)\n",
        "\n",
        "        x = self.pool1(x)\n",
        "        if self.transform:\n",
        "            self.features.append(self.transform2_0(x))\n",
        "        x = self.conv2_0(x)\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.conv2_2(x)\n",
        "\n",
        "        x = self.pool2(x)\n",
        "        if self.transform:\n",
        "            self.features.append(self.transform3_0(x))\n",
        "        x = self.conv3_0(x)\n",
        "        x = self.conv3_1(x)\n",
        "        x = self.conv3_2(x)\n",
        "\n",
        "        x = self.conv4_0(x)\n",
        "        if self.transform:\n",
        "            self.features.append(self.transform4_0(x))\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.conv4_3(x)\n",
        "        if self.transform:\n",
        "            self.features.append(self.transform4_3(x))\n",
        "        x = self.conv4_4(x)\n",
        "        x = self.conv4_5(x)\n",
        "\n",
        "        x = self.conv5_0(x)\n",
        "\n",
        "        self.features.append(x)\n",
        "\n",
        "        if self.training is True:\n",
        "            return self.features\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu').cuda()\n",
        "                # nn.init.normal_(m.weight, std=0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def conv_layers(inp, oup, dilation=False):\n",
        "    if dilation:\n",
        "        d_rate = 2\n",
        "    else:\n",
        "        d_rate = 1\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, kernel_size=3, padding=d_rate, dilation=d_rate),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def feature_transform(inp, oup):\n",
        "    conv2d = nn.Conv2d(inp, oup, kernel_size=1)  # no padding\n",
        "    relu = nn.ReLU(inplace=True)\n",
        "    layers = []\n",
        "    layers += [conv2d, relu]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def pool_layers(ceil_mode=True):\n",
        "    return nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=ceil_mode)"
      ],
      "metadata": {
        "id": "lGGI3FCYxfbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "train_list=[]\n",
        "model = CSRNet()\n",
        "\n",
        "dataset = listDataset(train_list,\n",
        "                      shuffle=True,\n",
        "                       transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Resize(768),\n",
        "                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                      std=[0.229, 0.224, 0.225]),\n",
        "                       ]),\n",
        "                      # target_transform=transforms.Compose([                                 \n",
        "                      #   transforms.Resize(768),\n",
        "                      #  ]),\n",
        "                       train=True, \n",
        "                       seen=model.seen,\n",
        "                       batch_size=batch_size,\n",
        "                       num_workers=workers)"
      ],
      "metadata": {
        "id": "PGUnErmRgpR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fad499d-1108-44f1-de6b-894ac3fb7d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = listDataset(train_list,\n",
        "                            transform=transforms.Compose([\n",
        "                            transforms.Resize(768),\n",
        "                            transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                   std=[0.229, 0.224, 0.225]),\n",
        "                            ]),\n",
        "                            #target_transform=transforms.Compose([                                 \n",
        "                             # transforms.Resize(768),\n",
        "                             # ]),\n",
        "                             train=True,\n",
        "                             seen=0\n",
        "                             )\n",
        "dl = torch.utils.data.DataLoader(\n",
        "         ds,\n",
        "         num_workers=workers,\n",
        "         shuffle=False,\n",
        "         batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-ATkstzZbsCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "CRmpo6H6YLJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSRNet_student()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb-XzWldagJP",
        "outputId": "71333e14-bb52-4d49-8a93-ad2c97c1a410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CSRNet_student(\n",
              "  (conv0_0): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (transform0_0): Sequential(\n",
              "    (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv0_1): Sequential(\n",
              "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (transform1_0): Sequential(\n",
              "    (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv1_0): Sequential(\n",
              "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv1_1): Sequential(\n",
              "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (transform2_0): Sequential(\n",
              "    (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv2_0): Sequential(\n",
              "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv2_1): Sequential(\n",
              "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv2_2): Sequential(\n",
              "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (transform3_0): Sequential(\n",
              "    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv3_0): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv3_1): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv3_2): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv4_0): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (transform4_0): Sequential(\n",
              "    (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv4_1): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv4_2): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv4_3): Sequential(\n",
              "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (transform4_3): Sequential(\n",
              "    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv4_4): Sequential(\n",
              "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv4_5): Sequential(\n",
              "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv5_0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "input=(3,768,786)\n",
        "summary(CSRNet_student(),input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "yhbnWUy-XA15",
        "outputId": "388808f6-43af-40ec-e3a0-168e53b12650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-7fd4c1d0a732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m786\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSRNet_student\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-160f4eaa0f9f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ratio, transform)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCSRNet_student\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSRNet_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cuda'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Distillation**"
      ],
      "metadata": {
        "id": "8-q8wqKuvnF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from torchvision import models\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "1wYWbhY-v0q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(stu_map, tea_map):\n",
        "    cs = nn.CosineSimilarity(dim=1)\n",
        "    similiar = 1-cs(stu_map, tea_map)\n",
        "    loss = similiar.sum()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cal_dense_fsp(features):\n",
        "    fsp = []\n",
        "    for groups in features:\n",
        "        for i in range(len(groups)):\n",
        "            for j in range(i+1, len(groups)):\n",
        "                x = groups[i]\n",
        "                y = groups[j]\n",
        "\n",
        "                norm1 = nn.InstanceNorm2d(x.shape[1])\n",
        "                norm2 = nn.InstanceNorm2d(y.shape[1])\n",
        "                x = norm1(x)\n",
        "                y = norm2(y)\n",
        "                res = gram(x, y)\n",
        "                fsp.append(res)\n",
        "    return fsp\n",
        "\n",
        "\n",
        "def gram(x, y):\n",
        "    n = x.shape[0]\n",
        "    c1 = x.shape[1]\n",
        "    c2 = y.shape[1]\n",
        "    h = x.shape[2]\n",
        "    w = x.shape[3]\n",
        "    x = x.view(n, c1, -1, 1)[0, :, :, 0]\n",
        "    y = y.view(n, c2, -1, 1)[0, :, :, 0]\n",
        "    y = y.transpose(0, 1)\n",
        "    # print x.shape\n",
        "    # print y.shape\n",
        "    z = torch.mm(x, y) / (w*h)\n",
        "    return z\n",
        "\n",
        "\n",
        "def scale_process(features, scale=[3, 2, 1], ceil_mode=True):\n",
        "    # process features for multi-scale dense fsp\n",
        "    new_features = []\n",
        "    for i in range(len(features)):\n",
        "        if i >= len(scale):\n",
        "            new_features.append(features[i])\n",
        "            continue\n",
        "        down_ratio = pow(2, scale[i])\n",
        "        pool = nn.MaxPool2d(kernel_size=down_ratio, stride=down_ratio, ceil_mode=ceil_mode)\n",
        "        new_features.append(pool(features[i]))\n",
        "    return new_features"
      ],
      "metadata": {
        "id": "aNaCUVEcva2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**writer**"
      ],
      "metadata": {
        "id": "iZR8Ao4X3jUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "b9luWFTR3jUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter('/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/CSRNet_models_weights/epoch=1000_3')"
      ],
      "metadata": {
        "id": "VLvIpkoz3jUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "hAqrJddL3jUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}