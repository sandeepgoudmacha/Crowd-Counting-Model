{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Structured Knowledge Transfer (SKT)**"
      ],
      "metadata": {
        "id": "85IPZ5E6tC9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Utils**"
      ],
      "metadata": {
        "id": "6tstS7G9tm41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "s_eddDH8uX5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_net(fname, net):\n",
        "    with h5py.File(fname, 'w') as h5f:\n",
        "        for k, v in net.state_dict().items():\n",
        "            h5f.create_dataset(k, data=v.cpu().numpy())\n",
        "\n",
        "\n",
        "def load_net(fname, net):\n",
        "    with h5py.File(fname, 'r') as h5f:\n",
        "        for k, v in net.state_dict().items():        \n",
        "            param = torch.from_numpy(np.asarray(h5f[k]))         \n",
        "            v.copy_(param)            \n",
        "path=\"/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/CSRNet_models_weights/checkpoint.pth.tar\"\n",
        "def save_checkpoint(state, mae_is_best, mse_is_best, path, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, os.path.join(path, filename))\n",
        "    epoch = state['epoch']\n",
        "    if mae_is_best:\n",
        "        shutil.copyfile(os.path.join(path, filename), os.path.join(path, 'epoch'+str(epoch)+'_best_mae.pth.tar'))\n",
        "    if mse_is_best:\n",
        "        shutil.copyfile(os.path.join(path, filename), os.path.join(path, 'epoch'+str(epoch)+'_best_mse.pth.tar'))\n",
        "\n",
        "\n",
        "def cal_para(net):\n",
        "    params = list(net.parameters())\n",
        "    k = 0\n",
        "    for i in params:\n",
        "        l = 1\n",
        "        # print \"stucture of layer: \" + str(list(i.size()))\n",
        "        for j in i.size():\n",
        "            l *= j\n",
        "        # print \"para in this layer: \" + str(l)\n",
        "        k = k + l\n",
        "    print(\"the amount of para: \" + str(k))\n",
        "\n",
        "\n",
        "def crop_img_patches(img, size=512):\n",
        "    \"\"\" crop the test images to patches\n",
        "    while testing UCF data, we load original images, then use crop_img_patches to crop the test images to patches,\n",
        "    calculate the crowd count respectively and sum them together finally\n",
        "    \"\"\"\n",
        "    w = img.shape[3]\n",
        "    h = img.shape[2]\n",
        "    x = int(w/size)+1\n",
        "    y = int(h/size)+1\n",
        "    crop_w = int(w/x)\n",
        "    crop_h = int(h/y)\n",
        "    patches = []\n",
        "    for i in range(x):\n",
        "        for j in range(y):\n",
        "            start_x = crop_w*i\n",
        "            if i == x-1:\n",
        "                end_x = w\n",
        "            else:\n",
        "                end_x = crop_w*(i+1)\n",
        "\n",
        "            start_y = crop_h*j\n",
        "            if j == y - 1:\n",
        "                end_y = h\n",
        "            else:\n",
        "                end_y = crop_h*(j+1)\n",
        "\n",
        "            sub_img = img[:, :, start_y:end_y, start_x:end_x]\n",
        "            patches.append(sub_img)\n",
        "    return patches"
      ],
      "metadata": {
        "id": "xTKFTtIwtGZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Image**"
      ],
      "metadata": {
        "id": "DiOyxM0yt8Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "from PIL import Image,ImageFilter,ImageDraw\n",
        "import numpy as np\n",
        "import h5py\n",
        "from PIL import ImageStat\n",
        "import cv2\n",
        "import time"
      ],
      "metadata": {
        "id": "3fxs9MrTt4dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(img_path,train=True, dataset='shanghai'):\n",
        "    \"\"\" Load data\n",
        "    Use crop_ratio between 0.5 and 1.0 for random crop\n",
        "    \"\"\"\n",
        "    gt_path = img_path.replace('.jpg', '.h5').replace('images', 'ground_truth')\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    gt_file = h5py.File(gt_path)\n",
        "    target = np.asarray(gt_file['density'])\n",
        "    if train:\n",
        "        if dataset == 'shanghai':\n",
        "            crop_ratio = random.uniform(0.5, 1.0)\n",
        "            crop_size = (int(crop_ratio*img.size[0]), int(crop_ratio*img.size[1]))\n",
        "            dx = int(random.random() * (img.size[0]-crop_size[0]))\n",
        "            dy = int(random.random() * (img.size[1]-crop_size[1]))\n",
        "\n",
        "            img = img.crop((dx,dy,crop_size[0]+dx,crop_size[1]+dy))\n",
        "            target = target[dy:crop_size[1]+dy,dx:crop_size[0]+dx]\n",
        "\n",
        "        if random.random() > 0.8:\n",
        "            target = np.fliplr(target)\n",
        "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "    target = reshape_target(target, 3)\n",
        "    target = np.expand_dims(target, axis=0)\n",
        "\n",
        "    img = img.copy()\n",
        "    target = target.copy()\n",
        "    return img, target\n",
        "\n",
        "\n",
        "def load_ucf_ori_data(img_path):\n",
        "    \"\"\" Load original UCF-QNRF data for testing\n",
        "    \"\"\"\n",
        "    gt_path = img_path.replace('.jpg', '.h5').replace('images', 'ground_truth')\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    gt_file = h5py.File(gt_path)\n",
        "    target = np.asarray(gt_file['density'])\n",
        "    return img, target\n",
        "\n",
        "\n",
        "def reshape_target(target, down_sample=3):\n",
        "    \"\"\" Down sample GT to 1/8\n",
        "    \"\"\"\n",
        "    height = target.shape[0]\n",
        "    width = target.shape[1]\n",
        "\n",
        "    # ceil_mode=True for nn.MaxPool2d in model\n",
        "    for i in range(down_sample):\n",
        "        height = int((height+1)/2)\n",
        "        width = int((width+1)/2)\n",
        "        # height = int(height/2)\n",
        "        # width = int(width/2)\n",
        "\n",
        "    target = cv2.resize(target, (width, height), interpolation=cv2.INTER_CUBIC) * (2**(down_sample*2))\n",
        "    return target"
      ],
      "metadata": {
        "id": "yr_94Yqwtmg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Dataset**"
      ],
      "metadata": {
        "id": "MFcGLXRINnf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(img_path,train = True):\n",
        "    gt_path = img_path.replace('.jpg','.npy').replace('images','ground_truth')\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    target = np.load(gt_path)\n",
        "    if False:\n",
        "        crop_size = (img.size[0]/2,img.size[1]/2)\n",
        "        if random.randint(0,9)<= -1:\n",
        "            dx = int(random.randint(0,1)*img.size[0]*1./2)\n",
        "            dy = int(random.randint(0,1)*img.size[1]*1./2)\n",
        "        else:\n",
        "            dx = int(random.random()*img.size[0]*1./2)\n",
        "            dy = int(random.random()*img.size[1]*1./2)\n",
        "        img = img.crop((dx,dy,crop_size[0]+dx,crop_size[1]+dy))\n",
        "        target = target[dy:crop_size[1]+dy,dx:crop_size[0]+dx]\n",
        "\n",
        "        if random.random()>0.8:\n",
        "            target = np.fliplr(target)\n",
        "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return img,target"
      ],
      "metadata": {
        "id": "7dtmKCMOeb-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "#from image import *\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "\n",
        "class listDataset(Dataset):\n",
        "    def __init__(self, root, shape=None, transform=None, shuffle=True,  train=False, seen=0,\n",
        "                 batch_size=1, num_workers=0, dataset='shanghai'):\n",
        "        if train and dataset == 'shanghai':\n",
        "            root = root*4\n",
        "        random.shuffle(root)\n",
        "        \n",
        "        self.nSamples = len(root)\n",
        "        self.lines = root\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "        self.shape = shape\n",
        "        self.seen = seen\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nSamples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        assert index <= len(self), 'index range error' \n",
        "        \n",
        "        img_path = self.lines[index]\n",
        "\n",
        "        if self.dataset == 'ucf_test':\n",
        "            # test in UCF\n",
        "            img, target = load_ucf_ori_data(img_path)\n",
        "        else:\n",
        "            img, target = load_data(img_path, self.train)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "            ratio = target.shape[0]/768\n",
        "            target = cv2.resize(target,(768,768),interpolation = cv2.INTER_CUBIC)*(ratio**2)\n",
        "            target = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
        "            target = torch.tensor(target, dtype=torch.float32).unsqueeze(0)\n",
        "        return img, target"
      ],
      "metadata": {
        "id": "TK0E2bxNNq61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_json = \"/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/part_A_final/json/part_A_train.json\"\n",
        "with open(train_json, 'r') as outfile:\n",
        "        train_list = json.load(outfile)\n",
        "workers = 0\n",
        "batch_size = 6"
      ],
      "metadata": {
        "id": "9-QUOdBxb1MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Preprocess**"
      ],
      "metadata": {
        "id": "kWotx_FxrqNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**make_json**"
      ],
      "metadata": {
        "id": "IFfA9wWOr5d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Make json files for dataset\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "def get_val(root):\n",
        "    \"\"\"\n",
        "    Validation set follows part_A_val.json in CSRNet\n",
        "    https://github.com/leeyeehoo/CSRNet-pytorch\n",
        "    \"\"\"\n",
        "    with open(\"/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset/part_A_final/json/part_A_val.json\") as f:\n",
        "        val_list = json.load(f)\n",
        "    new_val = []\n",
        "    for item in val_list:\n",
        "        new_item = item.replace('/home/leeyh/Downloads/Shanghai/', root)\n",
        "        new_val.append(new_item)\n",
        "    with open('A_val.json', 'w') as f:\n",
        "        json.dump(new_val, f)\n",
        "\n",
        "\n",
        "def get_train(root):\n",
        "    path = os.path.join(root, 'part_A_final', 'train_data', 'images')\n",
        "    filenames = os.listdir(path)\n",
        "    pathname = [os.path.join(path, filename) for filename in filenames]\n",
        "    with open('A_train.json', 'w') as f:\n",
        "        json.dump(pathname, f)\n",
        "\n",
        "\n",
        "def get_test(root):\n",
        "    path = os.path.join(root, 'part_A_final', 'test_data', 'images')\n",
        "    filenames = os.listdir(path)\n",
        "    pathname = [os.path.join(path, filename) for filename in filenames]\n",
        "    with open('A_test.json', 'w') as f:\n",
        "        json.dump(pathname, f)\n",
        "\n",
        "\n",
        "root = '/content/drive/MyDrive/ShanghaiTech_Crowd_Counting_Dataset'  # Dataset path\n",
        "get_train(root)\n",
        "get_val(root)\n",
        "get_test(root)\n",
        "print('Finish!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd1GhKtQrwmB",
        "outputId": "7589d867-adad-4502-8dc5-657d43c68ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish!\n"
          ]
        }
      ]
    }
  ]
}